{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa37e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (1.0.3)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (1.0.5)\n",
      "Requirement already satisfied: gigachat in d:\\aleks\\anaconda3\\lib\\site-packages (0.1.43)\n",
      "Requirement already satisfied: pdfplumber in d:\\aleks\\anaconda3\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: PyPDF2 in d:\\aleks\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-dotenv in d:\\aleks\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langgraph) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langgraph) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langgraph) (0.2.9)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langgraph) (2.12.4)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langgraph) (3.6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core) (0.4.43)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from langchain-core) (6.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from langchain-core) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: httpx<1 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from gigachat) (0.28.1)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in d:\\aleks\\anaconda3\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in d:\\aleks\\anaconda3\\lib\\site-packages (from pdfplumber) (9.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (41.0.3)\n",
      "Requirement already satisfied: anyio in d:\\aleks\\anaconda3\\lib\\site-packages (from httpx<1->gigachat) (3.5.0)\n",
      "Requirement already satisfied: certifi in d:\\aleks\\anaconda3\\lib\\site-packages (from httpx<1->gigachat) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1->gigachat) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\aleks\\anaconda3\\lib\\site-packages (from httpx<1->gigachat) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1->gigachat) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\aleks\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\aleks\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=2.7.4->langgraph) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\aleks\\appdata\\roaming\\python\\python311\\site-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\aleks\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aleks\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\aleks\\anaconda3\\lib\\site-packages (from anyio->httpx<1->gigachat) (1.2.0)\n",
      "Requirement already satisfied: pycparser in d:\\aleks\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph langchain-core gigachat pdfplumber PyPDF2 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71107ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\DS projects\\pd_agent_system\n",
      "['.env', '.ipynb_checkpoints', 'agent_solution.ipynb', 'resumes']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4131725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import pdfplumber\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from gigachat import GigaChat\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "GIGACHAT_API_KEY = os.getenv(\"GIGACHAT_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef621af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PROMPTS:\n",
    "    ROADMAP = \"\"\"–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∫–∞—Ä—å–µ—Ä–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç –≤ IT-—Å—Ñ–µ—Ä–µ.\n",
    "–°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—É—é –¥–æ—Ä–æ–∂–Ω—É—é –∫–∞—Ä—Ç—É –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏—é {profession} –¥–ª—è —É—Ä–æ–≤–Ω—è {goal}.\n",
    "–ù–∞—á–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {background}.\n",
    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:\n",
    "1. –ù–ê–ü–†–ê–í–õ–ï–ù–ò–ï –ò –£–†–û–í–ù–ò\n",
    "‚Ä¢ –¢–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "‚Ä¢ –¶–µ–ª–µ–≤–æ–π —É—Ä–æ–≤–µ–Ω—å\n",
    "‚Ä¢ –†–∞–∑—Ä—ã–≤ –≤ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è—Ö\n",
    "2. –°–†–û–ö–ò –û–ë–£–ß–ï–ù–ò–Ø\n",
    "‚Ä¢ –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω\n",
    "‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –≤ –Ω–µ–¥–µ–ª—é\n",
    "‚Ä¢ –≠—Ç–∞–ø—ã —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–µ–¥–ª–∞–π–Ω–∞–º–∏\n",
    "3. –®–ê–ì–ò –ü–û –û–ë–£–ß–ï–ù–ò–Æ (—Ä–∞–∑–±–∏—Ç—å –ø–æ –º–µ—Å—è—Ü–∞–º)\n",
    "‚Ä¢ –ú–µ—Å—è—Ü 1: –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ç–µ–º—ã\n",
    "‚Ä¢ –ú–µ—Å—è—Ü 2: –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ç–µ–º—ã\n",
    "‚Ä¢ –ú–µ—Å—è—Ü 3: –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ç–µ–º—ã\n",
    "‚Ä¢ –ú–µ—Å—è—Ü 4+: —É–≥–ª—É–±–ª–µ–Ω–∏–µ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "4. –ß–¢–û –ö–û–ù–ö–†–ï–¢–ù–û –£–ß–ò–¢–¨\n",
    "‚Ä¢ –Ø–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤–µ—Ä—Å–∏—è–º–∏\n",
    "‚Ä¢ –§—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "‚Ä¢ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –ü–û\n",
    "‚Ä¢ –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –∏ –ø—Ä–∞–∫—Ç–∏–∫–∏\n",
    "‚Ä¢ –¢–µ–æ—Ä–∏—è (–∞–ª–≥–æ—Ä–∏—Ç–º—ã, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ —Ç.–¥.)\n",
    "5. –†–ï–°–£–†–°–´ –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø\n",
    "‚Ä¢ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ: –∫—É—Ä—Å—ã, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è, YouTube-–∫–∞–Ω–∞–ª—ã\n",
    "‚Ä¢ –ü–ª–∞—Ç–Ω—ã–µ –∫—É—Ä—Å—ã (–µ—Å–ª–∏ —Å—Ç–æ–∏—Ç —Ç–æ–≥–æ)\n",
    "‚Ä¢ –ö–Ω–∏–≥–∏ —Å –∞–≤—Ç–æ—Ä–∞–º–∏ –∏ –∏–∑–¥–∞–Ω–∏—è–º–∏\n",
    "‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã (LeetCode, Codewars, Kaggle)\n",
    "‚Ä¢ –°–æ–æ–±—â–µ—Å—Ç–≤–∞ –∏ —á–∞—Ç—ã\n",
    "6. –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ï–ö–¢–´\n",
    "‚Ä¢ –ü—Ä–æ–µ–∫—Ç 1: —Å—Ç–µ–∫, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, —Å—Ä–æ–∫\n",
    "‚Ä¢ –ü—Ä–æ–µ–∫—Ç 2: —Å—Ç–µ–∫, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, —Å—Ä–æ–∫\n",
    "‚Ä¢ –ü—Ä–æ–µ–∫—Ç 3: —Å—Ç–µ–∫, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, —Å—Ä–æ–∫\n",
    "‚Ä¢ –ì–¥–µ —Ä–∞–∑–º–µ—â–∞—Ç—å (GitHub, Behance –∏ —Ç.–¥.)\n",
    "7. –ö–ê–ö –ò–°–ö–ê–¢–¨ –†–ê–ë–û–¢–£\n",
    "‚Ä¢ –ì–¥–µ –∏—Å–∫–∞—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–∞–π—Ç—ã)\n",
    "‚Ä¢ –ö–∞–∫ —Å–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—é–º–µ –ø–æ–¥ —ç—Ç—É –ø–æ–∑–∏—Ü–∏—é\n",
    "‚Ä¢ –ö–∞–∫–∏–µ –Ω–∞–≤—ã–∫–∏ –≤—ã–¥–µ–ª—è—Ç—å\n",
    "‚Ä¢ –ö–∞–∫ –≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è–º\n",
    "‚Ä¢ –ö–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∑–∞–¥–∞–≤–∞—Ç—å —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—é\n",
    "‚Ä¢ –°—Ä–µ–¥–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã –Ω–∞ —Ä—ã–Ω–∫–µ\n",
    "\n",
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞ –¥–∞–≤–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Å—Ä–æ–∫–∏.\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç–µ–∫–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –¥–ª—è —ç—Ç–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏.\n",
    "–î–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –∏–∑ –æ–ø—ã—Ç–∞ –Ω–∞–π–º–∞ –≤ IT.\n",
    "–í –∫–æ–Ω—Ü–µ —Å–¥–µ–ª–∞–π –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞: ¬´–†–µ–∞–ª—å–Ω–æ –∑–∞ 3-4 –º–µ—Å—è—Ü–∞¬ª, ¬´–¢—Ä–µ–±—É–µ—Ç 6+ –º–µ—Å—è—Ü–µ–≤¬ª, ¬´–°–ª–æ–∂–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥¬ª –∏ —Ç.–¥.\n",
    "\"\"\"\n",
    "\n",
    "    # –≠—Ç–æ—Ç –ø—Ä–æ–º–ø—Ç ‚Äî –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∑—é–º–µ/—à–∞–±–ª–æ–Ω–æ–≤/—Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö (—Å–æ–∑–¥–∞–Ω–∏–µ, –Ω–µ –∞–Ω–∞–ª–∏–∑)\n",
    "    RESUME_AND_COVER_TEMPLATES = \"\"\"–†–æ–ª—å:\n",
    "–¢—ã –∫–∞—Ä—å–µ—Ä–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –∏ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é —Ä–µ–∑—é–º–µ –¥–ª—è IT, –∞–Ω–∞–ª–∏—Ç–∏–∫–∏, –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ –ø—Ä–æ–¥—É–∫—Ç–∞ –∏ —Å–º–µ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π.\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—é–º–µ,\n",
    "–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ –∫–æ–º–ø–∞–Ω–∏–∏, —Å —É—á—ë—Ç–æ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Ä–µ–∫—Ä—É—Ç–µ—Ä–æ–≤ –∏ —Å–∏—Å—Ç–µ–º –æ—Ç–±–æ—Ä–∞ (ATS).\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:\n",
    "1) –ö—Ä–∞—Ç–∫–æ—Å—Ç—å –∏ —è—Å–Ω–æ—Å—Ç—å: –ø–æ–Ω—è—Ç–Ω–æ –∑–∞ 15‚Äì30 —Å–µ–∫—É–Ω–¥.\n",
    "2) –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –Ω–∞–≤–µ—Ä—Ö—É –∫–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏, –≤ –æ–ø—ã—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è.\n",
    "3) ATS: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏, —á–∏—Å—Ç—ã–π —Ñ–æ—Ä–º–∞—Ç, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã, —á—ë—Ç–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã.\n",
    "4) –ë–µ–∑ –ª–∏—à–Ω–µ–≥–æ: —Ñ–æ—Ç–æ/–≥—Ä–µ–π–¥/–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å (–µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è).\n",
    "5) ‚Äú–û —Å–µ–±–µ‚Äù: 2‚Äì3 —Å—Ç—Ä–æ–∫–∏, –±–µ–∑ –æ–±—â–∏—Ö —Ñ—Ä–∞–∑.\n",
    "\n",
    "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—é–º–µ:\n",
    "‚Ä¢ –§–ò–û\n",
    "‚Ä¢ –ö–æ–Ω—Ç–∞–∫—Ç—ã\n",
    "‚Ä¢ –ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏\n",
    "‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã (–¥–µ–π—Å—Ç–≤–∏–µ ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç, STAR)\n",
    "‚Ä¢ –ü—Ä–æ–µ–∫—Ç—ã\n",
    "‚Ä¢ –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\n",
    "‚Ä¢ –ö—É—Ä—Å—ã\n",
    "‚Ä¢ –Ø–∑—ã–∫–∏\n",
    "‚Ä¢ –û —Å–µ–±–µ\n",
    "\n",
    "–§–æ—Ä–º–∞—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:\n",
    "- –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Äî –∑–∞–ø—Ä–æ—Å–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ.\n",
    "- –ï—Å–ª–∏ –µ—Å—Ç—å –≤–∞–∫–∞–Ω—Å–∏—è/–∫–æ–º–ø–∞–Ω–∏—è ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—é –ø–æ–¥ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.\n",
    "- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –≤—ã–¥–∞–π —Ä–µ–∑—é–º–µ –≤ Markdown –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ + —Å–æ–≤–µ—Ç—ã –ø–æ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—é.\n",
    "\n",
    "–¢–æ–Ω: –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã.\n",
    "\"\"\"\n",
    "\n",
    "    DOC_ANALYSIS = \"\"\"–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–∞—Ä—å–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∏ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ —Ä–µ–∑—é–º–µ –∏ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∏—Å–µ–º.\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–≤–æ–¥–∏—Ç—å –≥–ª—É–±–æ–∫–∏–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.\n",
    "\n",
    "–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–¥–∞–≤–∞–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n",
    "- –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞ (2‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n",
    "- –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã\n",
    "- –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã\n",
    "- –†–∏—Å–∫–∏/—Å–æ–º–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—è\n",
    "- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é\n",
    "- –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–∞—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (–µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π)\n",
    "- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Å—Ç–∏–ª—é –∏ —Ç–æ–Ω—É\n",
    "\n",
    "–¢–æ—á–Ω–æ—Å—Ç—å:\n",
    "- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–∞–ª–æ ‚Äî –∑–∞–ø—Ä–∞—à–∏–≤–∞–π –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ.\n",
    "- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã.\n",
    "\n",
    "–°—Ç–∏–ª—å: —Å—Ç—Ä–æ–≥–∏–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã.\n",
    "\"\"\"\n",
    "\n",
    "    VACANCY_LEGAL_FLAGS = \"\"\"–¢—ã ‚Äî –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —é—Ä–∏—Å—Ç –ø–æ —Ç—Ä—É–¥–æ–≤–æ–º—É –ø—Ä–∞–≤—É —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π\n",
    "–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É (–¢–ö –†–§), –§–ó ‚Ññ59-–§–ó ¬´–û –∑–∞–ø—Ä–µ—Ç–µ –∞–Ω–æ–Ω–∏–º–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π¬ª –∏ –§–ó-152 ¬´–û –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö¬ª.\n",
    "\n",
    "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–∞–∫–∞–Ω—Å–∏—é –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:\n",
    "- –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏ (–ø–µ—Ä–µ—á–µ–Ω—å –∏–∑ –∑–∞–¥–∞–Ω–∏—è) + –ø–æ—è—Å–Ω–µ–Ω–∏—è:\n",
    "  –ø–æ—á–µ–º—É —ç—Ç–æ —Ñ–ª–∞–≥, –ø—Ä–∞–≤–æ–≤–æ–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–µ/–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞, —Å—Ç–∞—Ç—å—è –∑–∞–∫–æ–Ω–∞ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ).\n",
    "- –ó–µ–ª—ë–Ω—ã–µ —Ñ–ª–∞–≥–∏ (–ø–µ—Ä–µ—á–µ–Ω—å –∏–∑ –∑–∞–¥–∞–Ω–∏—è) + –ø–æ—è—Å–Ω–µ–Ω–∏—è.\n",
    "\n",
    "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∏—Ç–æ–≥–æ–≤—ã–π –≤–µ—Ä–¥–∏–∫—Ç:\n",
    "- —Å—Ç–æ–∏—Ç –ª–∏ –æ—Ç–∫–ª–∏–∫–∞—Ç—å—Å—è, –ø—Ä–∏ –∫–∞–∫–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö,\n",
    "- –∫–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∑–∞–¥–∞—Ç—å –Ω–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–∏,\n",
    "- –∏—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞: ¬´–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –æ—Ç–∫–ª–∏–∫—É¬ª, ¬´–¢—Ä–µ–±—É–µ—Ç –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏¬ª –∏–ª–∏ ¬´–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ - –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è¬ª.\n",
    "\n",
    "–í–æ—Ç –≤–∞–∫–∞–Ω—Å–∏—è:\n",
    "{vacancy_text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fb2916",
   "metadata": {},
   "outputs": [],
   "source": [
    " def parse_kv(text: str) -> Dict[str, str]:\n",
    "    out = {}\n",
    "    for line in text.splitlines():\n",
    "        if \":\" in line:\n",
    "            k, v = line.split(\":\", 1)\n",
    "            out[k.strip().lower()] = v.strip()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7496c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#–°–æ—Å—Ç–æ—è–Ω–∏–µ –≥—Ä–∞—Ñ–∞\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Optional\n",
    "import pdfplumber\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# @dataclass state –æ–∫, –Ω–æ langgraph —á–∞—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict ‚Äî –ø–æ—ç—Ç–æ–º—É –¥–∞–ª–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º getattr/–ø–æ–ª—è –∞–∫–∫—É—Ä–∞—Ç–Ω–æ\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    messages: List[Dict[str, Any]]\n",
    "    current_agent: str\n",
    "    user_query: str\n",
    "    extracted_text: Optional[str] = None\n",
    "    final_response: Optional[str] = None\n",
    "\n",
    "\n",
    "#  –ë–∞–∑–æ–≤—ã–π –∞–≥–µ–Ω—Ç\n",
    "class BaseAgent:\n",
    "    def __init__(self, name: str, tools: List[Any]):\n",
    "        self.name = name\n",
    "        self.tools = tools\n",
    "        self.model = GigaChat(credentials=GIGACHAT_API_KEY, verify_ssl_certs=False)\n",
    "\n",
    "    def chat(self, user_query: str, history: Optional[List[Dict[str, str]]] = None) -> str:\n",
    "        \"\"\"\n",
    "        –û–±—Ä–∞—â–µ–Ω–∏–µ –∫ LLM —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞.\n",
    "        history: [{\"role\": \"user\"/\"assistant\", \"content\": \"...\"}]\n",
    "        \"\"\"\n",
    "        if history:\n",
    "            hist_text = \"\"\n",
    "            for msg in history[-10:]:\n",
    "                role = \"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å\" if msg[\"role\"] == \"user\" else \"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç\"\n",
    "                hist_text += f\"{role}: {msg['content']}\\n\"\n",
    "\n",
    "            prompt = (\n",
    "                f\"{hist_text}\\n\"\n",
    "                f\"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_query}\\n\"\n",
    "                f\"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:\"\n",
    "            )\n",
    "        else:\n",
    "            prompt = user_query\n",
    "\n",
    "        resp = self.model.chat(prompt)\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "    # ‚úÖ –í—ã–Ω–µ—Å–ª–∏ PDF-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –≤ –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å, —á—Ç–æ–±—ã –æ–±–∞ –∞–≥–µ–Ω—Ç–∞ –º–æ–≥–ª–∏ —á–∏—Ç–∞—Ç—å PDF\n",
    "    def extract_pdf_text_tool(self, pdf_path: str) -> str:\n",
    "        \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞\"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text += (page.extract_text() or \"\") + \"\\n\"\n",
    "            return text[:4000]  # –ª–∏–º–∏—Ç —á—É—Ç—å —Ä–∞—Å—à–∏—Ä–∏–º\n",
    "        except Exception as e:\n",
    "            return f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {str(e)}\"\n",
    "\n",
    "class OrchestratorAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"orchestrator\", [])\n",
    "\n",
    "    def route_query(self, state: AgentState) -> str:\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–æ–π –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å\"\"\"\n",
    "        query = state.user_query.lower()\n",
    "\n",
    "        career_prep_keywords = [\n",
    "            \"—Ä–µ–∑—é–º–µ\", \"cv\", \"roadmap\", \"–∫–∞—Ä—å–µ—Ä\", \"—Å–æ–ø—Ä–æ–≤–æ–¥\",\n",
    "            \"–ø–∏—Å—å–º–æ\", \"—à–∞–±–ª–æ–Ω\", \"–ø–æ–¥–≥–æ—Ç–æ–≤\"\n",
    "        ]\n",
    "        hiring_keywords = [\n",
    "            \"–≤–∞–∫–∞–Ω—Å–∏\", \"—Å–æ–±–µ—Å–µ–¥\", \"–∏–Ω—Ç–µ—Ä–≤—å—é\",\n",
    "            \"–≤–æ–ø—Ä–æ—Å\", \"red flag\", \"green flag\"\n",
    "        ]\n",
    "\n",
    "        prep_score = sum(1 for k in career_prep_keywords if k in query)\n",
    "        hiring_score = sum(1 for k in hiring_keywords if k in query)\n",
    "\n",
    "        if prep_score >= hiring_score:\n",
    "            return \"career_preparation_agent\"\n",
    "        else:\n",
    "            return \"hiring_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65878ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CareerPreparationAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        tools = [\n",
    "            self.create_roadmap_tool,\n",
    "            self.analyze_resume_tool,\n",
    "            self.generate_resume_or_templates_tool,\n",
    "            self.generate_cover_letter_tool,\n",
    "            self.extract_pdf_text_tool,\n",
    "        ]\n",
    "        super().__init__(\"career_preparation_agent\", tools)\n",
    "\n",
    "    def create_roadmap_tool(self, profession: str, goal: str, background: str, history=None) -> str:\n",
    "        prompt = PROMPTS.ROADMAP.format(\n",
    "            profession=profession,\n",
    "            goal=goal,\n",
    "            background=background\n",
    "        )\n",
    "        return self.chat(prompt, history=history)\n",
    "\n",
    "    def generate_resume_or_templates_tool(self, user_data: str, vacancy_text: Optional[str] = None, history=None) -> str:\n",
    "        # user_data ‚Äî —Ç–æ, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ —Å–µ–±—è –¥–∞–ª (–Ω–∞–≤—ã–∫–∏/–æ–ø—ã—Ç/–ø—Ä–æ–µ–∫—Ç—ã)\n",
    "        extra = \"\"\n",
    "        if vacancy_text:\n",
    "            extra = f\"\\n\\n–í–∞–∫–∞–Ω—Å–∏—è/—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–π –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è ATS):\\n{vacancy_text}\\n\"\n",
    "        prompt = f\"{PROMPTS.RESUME_AND_COVER_TEMPLATES}\\n\\n–î–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\\n{user_data}\\n{extra}\\n\\n–í—ã–¥–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\"\n",
    "        return self.chat(prompt, history=history)\n",
    "\n",
    "    def analyze_resume_tool(self, pdf_path: str, history=None) -> str:\n",
    "        text = self.extract_pdf_text_tool(pdf_path)\n",
    "        prompt = f\"{PROMPTS.DOC_ANALYSIS}\\n\\n–¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ:\\n{text}\\n\\n–í—ã–¥–∞–π –ø–æ–ª—É—á–∏–≤—à–∏–π—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\"\n",
    "        return self.chat(prompt, history=history)\n",
    "\n",
    "    def analyze_cover_letter_text_tool(self, cover_letter_text: str, history=None) -> str:\n",
    "        prompt = f\"{PROMPTS.DOC_ANALYSIS}\\n\\n–¢–µ–∫—Å—Ç —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞:\\n{cover_letter_text}\\n\\n–í—ã–¥–∞–π –ø–æ–ª—É—á–∏–≤—à–∏–π—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\"\n",
    "        return self.chat(prompt, history=history)\n",
    "\n",
    "    def generate_cover_letter_tool(self, position: str, company: str, user_profile: str = \"\", history=None) -> str:\n",
    "        # –õ–∞–∫–æ–Ω–∏—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä; –µ—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî —Ç–æ–∂–µ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–º –±–æ–ª—å—à–∏–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
    "        prompt = f\"\"\"{PROMPTS.RESUME_AND_COVER_TEMPLATES}\n",
    "        –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ –Ω–∞ –ø–æ–∑–∏—Ü–∏—é \"{position}\" –≤ –∫–æ–º–ø–∞–Ω–∏—é \"{company}\".\n",
    "        –î–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–µ—Å–ª–∏ –µ—Å—Ç—å):\n",
    "        {user_profile}\n",
    "        –§–æ—Ä–º–∞—Ç: Markdown. –ë–µ–∑ –≤–æ–¥—ã. 180‚Äì260 —Å–ª–æ–≤.\n",
    "        \"\"\"\n",
    "        return self.chat(prompt, history=history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d232aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiringAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        tools = [\n",
    "            self.analyze_vacancy_text_tool,\n",
    "            self.analyze_vacancy_pdf_tool,\n",
    "            self.generate_interview_questions_tool,\n",
    "            self.extract_pdf_text_tool,\n",
    "        ]\n",
    "        super().__init__(\"hiring_agent\", tools)\n",
    "\n",
    "    def analyze_vacancy_text_tool(self, vacancy_text: str, history=None) -> str:\n",
    "        prompt = PROMPTS.VACANCY_LEGAL_FLAGS.format(vacancy_text=vacancy_text)\n",
    "        return self.chat(prompt, history=history)\n",
    "\n",
    "    def analyze_vacancy_pdf_tool(self, pdf_path: str, history=None) -> str:\n",
    "        text = self.extract_pdf_text_tool(pdf_path)\n",
    "        prompt = PROMPTS.VACANCY_LEGAL_FLAGS.format(vacancy_text=text)\n",
    "        return self.chat(prompt, history=history)\n",
    "\n",
    "    def generate_interview_questions_tool(self, position: str, level: str = \"middle\", history=None) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –∏–∑ 10-15 –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–æ–∑–∏—Ü–∏—é {position} —É—Ä–æ–≤–Ω—è {level}.\n",
    "        –í–∫–ª—é—á–∏:\n",
    "        - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã\n",
    "        - –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã\n",
    "        - –í–æ–ø—Ä–æ—Å—ã –æ –º–æ—Ç–∏–≤–∞—Ü–∏–∏\n",
    "        - –ö–µ–π—Å–æ–≤—ã–µ –∑–∞–¥–∞–Ω–∏—è\n",
    "        –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —É–∫–∞–∂–∏, –Ω–∞ —á—Ç–æ –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ.\n",
    "        \"\"\"\n",
    "        return self.chat(prompt, history=history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59380f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentSystem:\n",
    "    def __init__(self):\n",
    "        self.orchestrator = OrchestratorAgent()\n",
    "        self.career_preparation_agent = CareerPreparationAgent()\n",
    "        self.hiring_agent = HiringAgent()\n",
    "\n",
    "        # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ (—É —Ç–µ–±—è –µ—ë –Ω–µ –±—ã–ª–æ)\n",
    "        self.chat_history: List[Dict[str, str]] = []\n",
    "\n",
    "        self.graph = self._build_graph()\n",
    "    def _last_assistant_message(self) -> str:\n",
    "        for msg in reversed(self.chat_history):\n",
    "            if msg[\"role\"] == \"assistant\":\n",
    "                return msg[\"content\"]\n",
    "        return \"\"\n",
    "\n",
    "    def _append_history(self, user_query: str, response: str) -> None:\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "        self.chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞\n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        workflow.add_node(\"orchestrator\", self._orchestrator_node)\n",
    "        workflow.add_node(\"career_preparation_agent\", self._career_preparation_agent_node)\n",
    "        workflow.add_node(\"hiring_agent\", self._hiring_agent_node)\n",
    "\n",
    "        workflow.set_entry_point(\"orchestrator\")\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"orchestrator\",\n",
    "            self._route_after_orchestrator,\n",
    "            {\n",
    "                \"career_preparation_agent\": \"career_preparation_agent\",\n",
    "                \"hiring_agent\": \"hiring_agent\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        workflow.add_edge(\"career_preparation_agent\", END)\n",
    "        workflow.add_edge(\"hiring_agent\", END)\n",
    "\n",
    "        return workflow.compile()\n",
    "\n",
    "    # –£–∑–ª—ã –≥—Ä–∞—Ñ–∞\n",
    "    def _orchestrator_node(self, state: AgentState) -> AgentState:\n",
    "        agent_type = self.orchestrator.route_query(state)\n",
    "        state.current_agent = agent_type\n",
    "        return state\n",
    "\n",
    "    def _career_preparation_agent_node(self, state: AgentState) -> AgentState:\n",
    "        response = self._process_with_agent(self.career_preparation_agent, state.user_query, self.chat_history)\n",
    "        state.final_response = response\n",
    "        return state\n",
    "\n",
    "    def _hiring_agent_node(self, state: AgentState) -> AgentState:\n",
    "        # ‚úÖ –±—ã–ª–æ self._hiring_agent (–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
    "        response = self._process_with_agent(self.hiring_agent, state.user_query, self.chat_history)\n",
    "        state.final_response = response\n",
    "        return state\n",
    "\n",
    "    def _route_after_orchestrator(self, state: AgentState) -> str:\n",
    "        return state.current_agent\n",
    "\n",
    "    # ‚úÖ –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–æ –ø–æ–¥ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É\n",
    "    def _process_with_agent(self, agent: BaseAgent, query: str, history: Optional[List[Dict[str, str]]] = None) -> str:\n",
    "        q = query.lower().strip()\n",
    "        \n",
    "        # --- CareerPreparationAgent ---\n",
    "        if isinstance(agent, CareerPreparationAgent):\n",
    "            # Roadmap intent\n",
    "            if \"roadmap\" in q or \"–¥–æ—Ä–æ–∂–Ω\" in q or \"–ø–ª–∞–Ω\" in q:\n",
    "                data = parse_kv(query)\n",
    "\n",
    "                # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ \"–∫–ª—é—á: –∑–Ω–∞—á–µ–Ω–∏–µ\"\n",
    "                if {\"profession\", \"goal\", \"background\"}.issubset(data.keys()):\n",
    "                    return agent.create_roadmap_tool(\n",
    "                        profession=data[\"profession\"],\n",
    "                        goal=data[\"goal\"],\n",
    "                        background=data[\"background\"],\n",
    "                        history=history\n",
    "                    )\n",
    "\n",
    "                # –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî –ø—Ä–æ—Å–∏–º –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º\n",
    "                return (\n",
    "                    \"–ß—Ç–æ–±—ã —Å–æ—Å—Ç–∞–≤–∏—Ç—å –¥–æ—Ä–æ–∂–Ω—É—é –∫–∞—Ä—Ç—É, –ø—Ä–∏—à–ª–∏ 3 —Å—Ç—Ä–æ–∫–∏:\\n\"\n",
    "                    \"profession: ...\\n\"\n",
    "                    \"goal: ...\\n\"\n",
    "                    \"background: ...\\n\"\n",
    "                )\n",
    "            # –ê–Ω–∞–ª–∏–∑ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ (–µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—Å—Ç–∞–≤–∏–ª —Ç–µ–∫—Å—Ç)\n",
    "            if \"—Å–æ–ø—Ä–æ–≤–æ–¥\" in q and (\"–∞–Ω–∞–ª–∏–∑\" in q or \"—Ä–∞–∑–±–æ—Ä\" in q or \"–æ—Ü–µ–Ω\" in q):\n",
    "                return (\n",
    "                    \"–í—Å—Ç–∞–≤—å —Å—é–¥–∞ —Ç–µ–∫—Å—Ç —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞ (–ø–æ–ª–Ω–æ—Å—Ç—å—é), –∏ —è —Å–¥–µ–ª–∞—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–±–æ—Ä.\"\n",
    "                )\n",
    "\n",
    "            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ/—à–∞–±–ª–æ–Ω–∞\n",
    "            if \"—à–∞–±–ª–æ–Ω\" in q or \"—Ä–µ–∑—é–º–µ\" in q and (\"—Å–≥–µ–Ω–µ—Ä\" in q or \"—Å–æ—Å—Ç–∞–≤\" in q or \"–Ω–∞–ø–∏—Å\" in q):\n",
    "                return (\n",
    "                    \"–ß—Ç–æ–±—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ/—à–∞–±–ª–æ–Ω, –ø—Ä–∏—à–ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º:\\n\"\n",
    "                    \"- —Ü–µ–ª—å (—Ä–æ–ª—å + –∫–æ–º–ø–∞–Ω–∏—è/–≤–∞–∫–∞–Ω—Å–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å)\\n\"\n",
    "                    \"- hard skills\\n\"\n",
    "                    \"- –æ–ø—ã—Ç (–ø–µ—Ä–∏–æ–¥, –¥–æ–ª–∂–Ω–æ—Å—Ç—å, –∫–æ–º–ø–∞–Ω–∏—è, –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è)\\n\"\n",
    "                    \"- –ø—Ä–æ–µ–∫—Ç—ã (—á—Ç–æ —Å–¥–µ–ª–∞–ª, —Å—Ç–µ–∫, —Ä–µ–∑—É–ª—å—Ç–∞—Ç)\\n\"\n",
    "                    \"- –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/–∫—É—Ä—Å—ã\\n\"\n",
    "                    \"- —è–∑—ã–∫–∏\\n\"\n",
    "                    \"- 2‚Äì3 —Å—Ç—Ä–æ–∫–∏ ‚Äú–æ —Å–µ–±–µ‚Äù (–µ—Å–ª–∏ –µ—Å—Ç—å)\\n\\n\"\n",
    "                    \"–ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ ‚Äî –¥–æ–±–∞–≤—å, –∞–¥–∞–ø—Ç–∏—Ä—É—é –ø–æ–¥ ATS.\"\n",
    "                )\n",
    "\n",
    "            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "            return agent.chat(query, history=history)\n",
    "\n",
    "        # --- HiringAgent ---\n",
    "        if isinstance(agent, HiringAgent):\n",
    "            last_assistant = self._last_assistant_message()\n",
    "            waiting_for_vacancy_text = (\n",
    "                \"–≤—Å—Ç–∞–≤—å —Å—é–¥–∞ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏\" in last_assistant.lower()\n",
    "                or \"–ø—Ä–∏—à–ª–∏ –ø—É—Ç—å –∫ pdf\" in last_assistant.lower()\n",
    "            )\n",
    "\n",
    "            # ‚úÖ –∫–ª—é—á–µ–≤–æ–π —Ñ–∏–∫—Å: –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞\n",
    "            if waiting_for_vacancy_text:\n",
    "                if \".pdf\" in query.lower().strip():\n",
    "                    return agent.analyze_vacancy_pdf_tool(query.strip(), history=history)\n",
    "                return agent.analyze_vacancy_text_tool(query, history=history)\n",
    "\n",
    "            # –æ–±—ã—á–Ω—ã–π single-turn\n",
    "            if \"–≤–∞–∫–∞–Ω—Å–∏\" in q or \"red flag\" in q or \"green flag\" in q:\n",
    "                if \".pdf\" in q:\n",
    "                    return agent.analyze_vacancy_pdf_tool(query.strip(), history=history)\n",
    "                return \"–í—Å—Ç–∞–≤—å —Å—é–¥–∞ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Ü–µ–ª–∏–∫–æ–º (–∏–ª–∏ –ø—Ä–∏—à–ª–∏ –ø—É—Ç—å –∫ PDF), –∏ —è —Å–¥–µ–ª–∞—é —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä red/green flags.\"\n",
    "\n",
    "            if \"–≤–æ–ø—Ä–æ—Å\" in q or \"—Å–æ–±–µ—Å–µ–¥\" in q or \"–∏–Ω—Ç–µ—Ä–≤—å—é\" in q:\n",
    "                return agent.generate_interview_questions_tool(position=query, level=\"middle\", history=history)\n",
    "\n",
    "            return agent.chat(query, history=history)\n",
    "\n",
    "\n",
    "\n",
    "    def _purpose_message(self) -> str:\n",
    "        return (\n",
    "            \"–Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∫–∞—Ä—å–µ—Ä–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏/–ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É.\\n\\n\"\n",
    "            \"–ú–æ–≥—É:\\n\"\n",
    "            \"‚Ä¢ —Å–æ—Å—Ç–∞–≤–ª—è—Ç—å roadmap —Ä–∞–∑–≤–∏—Ç–∏—è,\\n\"\n",
    "            \"‚Ä¢ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ –∏ —É–ª—É—á—à–∞—Ç—å –µ–≥–æ,\\n\"\n",
    "            \"‚Ä¢ –ø–∏—Å–∞—Ç—å —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∏—Å—å–º–∞,\\n\"\n",
    "            \"‚Ä¢ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–∞ red/green flags,\\n\"\n",
    "            \"‚Ä¢ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—é.\\n\"\n",
    "        )\n",
    "\n",
    "    def process_query(self, user_query: str, resume_file_path: Optional[str] = None) -> str:\n",
    "        q = user_query.lower()\n",
    "\n",
    "        # —Ñ–∏–ª—å—Ç—Ä ‚Äú–Ω–µ –ø–æ —Ç–µ–º–µ‚Äù –º–æ–∂–µ—à—å –æ—Å—Ç–∞–≤–∏—Ç—å, –Ω–æ —è –¥–µ–ª–∞—é –ø—Ä–æ—â–µ –∏ –º—è–≥—á–µ\n",
    "        base_keywords = [\n",
    "            \"—Ä–µ–∑—é–º–µ\", \"cv\", \"roadmap\", \"–∫–∞—Ä—å–µ—Ä\", \"—Å–æ–ø—Ä–æ–≤–æ–¥\", \"–ø–∏—Å—å–º–æ\",\n",
    "            \"–≤–∞–∫–∞–Ω—Å–∏\", \"—Å–æ–±–µ—Å–µ–¥\", \"–∏–Ω—Ç–µ—Ä–≤—å—é\", \"–≤–æ–ø—Ä–æ—Å\", \"red flag\", \"green flag\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # —Å–ø–µ—Ü-–∫–µ–π—Å: –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –ø–æ PDF\n",
    "        resume_keywords = ['—Ä–µ–∑—é–º–µ', 'cv']\n",
    "        analyze_keywords = ['–∞–Ω–∞–ª–∏–∑', '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π', '–æ—Ü–µ–Ω', '—Ä–∞–∑–±–æ—Ä']\n",
    "\n",
    "        is_resume_query = any(k in q for k in resume_keywords)\n",
    "        is_analyze_request = any(k in q for k in analyze_keywords)\n",
    "\n",
    "        if is_resume_query and is_analyze_request and resume_file_path is not None:\n",
    "            # ‚úÖ –±—ã–ª–æ self.career_agent\n",
    "            response = self.career_preparation_agent.analyze_resume_tool(resume_file_path)\n",
    "            self._append_history(user_query, response)\n",
    "            return response\n",
    "\n",
    "        initial_state = AgentState(\n",
    "            messages=[],\n",
    "            current_agent=\"orchestrator\",\n",
    "            user_query=user_query,\n",
    "        )\n",
    "        result = self.graph.invoke(initial_state)\n",
    "        response = result.get(\"final_response\", \"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å\") if isinstance(result, dict) else getattr(result, \"final_response\", None)\n",
    "        response = response or \"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å\"\n",
    "\n",
    "        self._append_history(user_query, response)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f8aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢—ã: —Å–æ—Å—Ç–∞–≤—å roadmap –ø–æ –∏–∑—É—á–µ–Ω–∏—é –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏\n",
      "–ë–æ—Ç: –ß—Ç–æ–±—ã —Å–æ—Å—Ç–∞–≤–∏—Ç—å –¥–æ—Ä–æ–∂–Ω—É—é –∫–∞—Ä—Ç—É, –ø—Ä–∏—à–ª–∏ 3 —Å—Ç—Ä–æ–∫–∏:\n",
      "profession: ...\n",
      "goal: ...\n",
      "background: ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "–¢—ã: –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫, —è ds —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, —Ö–æ—á—É –æ–≤–ª–∞–¥–µ—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π. –†–∞–±–æ—Ç–∞—é middle ds\n",
      "–ë–æ—Ç: ### Roadmap –ø–æ –æ—Å–≤–æ–µ–Ω–∏—é –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ (Roadmap –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏)\n",
      "\n",
      "#### Profession: –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫  \n",
      "**Goal:** –û–≤–ª–∞–¥–µ–Ω–∏–µ –≤—Å–µ–º–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–Ω–∞–Ω–∏–π –≤ —Ç–µ–∫—É—â—É—é —Ä–æ–ª—å Middle DS —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞.  \n",
      "**Background:** –ï—Å—Ç—å –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã Data Scientist —Å—Ä–µ–¥–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è, —Ö–æ—Ä–æ—à–æ –≤–ª–∞–¥–µ—é Python, SQL, —É–º–µ—é —Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–∞–±–æ—Ç–∞—é —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ BI –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (Power BI, Tableau). –ò–º–µ—é—Ç—Å—è –∑–Ω–∞–Ω–∏—è –æ—Å–Ω–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏, –æ–¥–Ω–∞–∫–æ –Ω—É–∂–Ω–æ —É–≥–ª—É–±–∏—Ç—å—Å—è –∏ –æ—Å–≤–æ–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏  \n",
      "### –ù–µ–¥–µ–ª—è 1‚Äì4  \n",
      "- **–°—É—Ç—å –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏:** –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–π –∏ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–∞. –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è: User Journey, LTV, CAC, Retention Rate, ARPU/ARPPU, Churn Rate, AOV, Customer Acquisition Cost.\n",
      "- **–ü–æ–Ω—è—Ç–∏—è customer journey:** —ç—Ç–∞–ø—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø—Ä–æ–¥—É–∫—Ç–æ–º (acquisition, activation, retention, revenue, advocacy).\n",
      "- **–ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤:** user acquisition, engagement metrics, conversion rates, monetization, retention.\n",
      "- **–ú–∞—Ç–µ—Ä–∏–∞–ª—ã:**  \n",
      "  üìö Kiselyov V., ¬´Product Analytics & Growth Hacking¬ª,  \n",
      "  üìñ Aarron Walters, ¬´The Art of Product Management¬ª,  \n",
      "  üì∫ YouTube-–∫–∞–Ω–∞–ª—ã Growth Hacker TV, Inspired By Data, Steli Efti.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏  \n",
      "### –ù–µ–¥–µ–ª–∏ 5‚Äì8  \n",
      "- **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:** Google Analytics, Mixpanel, Amplitude, Heap, Looker, Snowplow, Fathom Analytics.\n",
      "- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤:** –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–∫–∏–Ω–≥–∞ —Å–æ–±—ã—Ç–∏–π, –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å, —Å–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏—Ç–æ—Ä–∏–∏, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–µ—Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–∞ –∏ AB-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
      "- **–†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏:** –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ SQL-–∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏–∑ BI-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–≥–æ—Ä—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.\n",
      "- **–ú–∞—Ç–µ—Ä–∏–∞–ª—ã:**  \n",
      "  üìö Alan Dye, ¬´Building Data Products¬ª,  \n",
      "  üìñ David Singleton, ¬´Practical Web Analytics¬ª,  \n",
      "  üì∫ YouTube-–∫–∞–Ω–∞–ª—ã TechSmith Analytics, FullStory, GrowthHackers Community.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. –û—Å–Ω–æ–≤—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏  \n",
      "### –ù–µ–¥–µ–ª–∏ 9‚Äì12  \n",
      "- **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:** –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–∏–ø–æ—Ç–µ–∑, –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π –≥—Ä—É–ø–ø—ã, –∫–æ–Ω—Ç—Ä–æ–ª—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.\n",
      "- **–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è AB —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:** RFM-–∞–Ω–∞–ª–∏–∑, Split Testing, Multi-Armed Bandit, Online Controlled Experiments.\n",
      "- **–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:** T-test, ANOVA, Z-test, Power Analysis, Bayesian Statistics.\n",
      "- **–ú–∞—Ç–µ—Ä–∏–∞–ª—ã:**  \n",
      "  üìö Desney Tan, ¬´Experimentation: The Science of Learning Fast¬ª,  \n",
      "  üìñ Evan Goldstein, ¬´AB Testing and Optimization for the Web¬ª,  \n",
      "  üì∫ YouTube-–∫–∞–Ω–∞–ª—ã Optimizely Academy, Smashing Magazine, Content Marketing Institute.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π  \n",
      "### –ù–µ–¥–µ–ª–∏ 13‚Äì16  \n",
      "- **–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:** –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.\n",
      "- **–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:** –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤, –æ—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç–∞ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏.\n",
      "- **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–µ:** —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π ML, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.\n",
      "- **–ú–∞—Ç–µ—Ä–∏–∞–ª—ã:**  \n",
      "  üìö Alberto Romero, ¬´Predictive Analytics with Machine Learning¬ª,  \n",
      "  üìñ Anna Cavender, ¬´Big Data Analytics and Predictive Modeling¬ª,  \n",
      "  üì∫ YouTube-–∫–∞–Ω–∞–ª—ã Coursera ML courses, Microsoft Azure AI tutorials, OpenDataScience.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –∫–æ–º–∞–Ω–¥–æ–π –ø—Ä–æ–¥—É–∫—Ç–∞  \n",
      "### –ù–µ–¥–µ–ª–∏ 17‚Äì20  \n",
      "- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è:** –ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º –ø—Ä–æ–¥—É–∫—Ç–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É –∫–æ–º–ø–∞–Ω–∏–∏.\n",
      "- **Workshops –∏ –≤—Å—Ç—Ä–µ—á–∏:** –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π, —Ç—Ä–µ–Ω–∏–Ω–≥–∏ –ø–æ –ø–æ–≤—ã—à–µ–Ω–∏—é –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤.\n",
      "- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏ –∫–æ–º–∞–Ω–¥—ã –∏ –∑–∞–∫–∞–∑—á–∏–∫–∞:** –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞ –º–µ—Ç—Ä–∏–∫ –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –±–∏–∑–Ω–µ—Å–∞.\n",
      "- **–ú–∞—Ç–µ—Ä–∏–∞–ª—ã:**  \n",
      "  üìö Aaron Kim, ¬´Business Communication Skills¬ª,  \n",
      "  üìñ Robin Goldbach, ¬´Effective Business Presentations¬ª,  \n",
      "  üì∫ YouTube-–∫–∞–Ω–∞–ª—ã Agile Alliance, Salesforce Training, Pexels TV.\n",
      "\n",
      "---\n",
      "\n",
      "## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –∏ –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤  \n",
      "### –ù–µ–¥–µ–ª–∏ 21‚Äì24  \n",
      "- **–ò—Ç–æ–≥–æ–≤–æ–µ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ –ø—Ä–æ–µ–∫—Ç–æ–≤:** —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ, –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞–≤—ã–∫–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã.\n",
      "- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–±–æ—Ç–∞:** —É—á–∞—Å—Ç–∏–µ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö –≤–Ω—É—Ç—Ä–∏ –∫–æ–º–ø–∞–Ω–∏–∏, —Ä–∞–±–æ—Ç–∞ –Ω–∞–¥ —É–ª—É—á—à–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–∞.\n",
      "- **–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞:** —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∞ –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –ø—Ä–æ–π–¥–µ–Ω–Ω–æ–≥–æ –ø—É—Ç–∏, –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —Ü–µ–ª–µ–π –∏ –∑–∞–¥–∞—á.\n",
      "\n",
      "---\n",
      "\n",
      "–≠—Ç–æ—Ç roadmap –ø–æ–º–æ–∂–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Å–≤–æ–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ —É—Å–ø–µ—à–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ –∫–æ–º–∞–Ω–¥—É –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, –¥–æ–ø–æ–ª–Ω–∏–≤ —É–∂–µ –∏–º–µ—é—â–∏–π—Å—è –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã Data Scientist-–∞.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = MultiAgentSystem()\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"–¢—ã: \")\n",
    "    if user_text.strip().lower() in {\"–≤—ã—Ö–æ–¥\", \"exit\", \"quit\"}:\n",
    "        break\n",
    "\n",
    "    resume_path = None\n",
    "    q = user_text.lower()\n",
    "    if \"—Ä–µ–∑—é–º–µ\" in q and \"–∞–Ω–∞–ª–∏–∑\" in q:\n",
    "        resume_path = input(\"üìÇ –í–≤–µ–¥–∏ –ø—É—Ç—å –∫ PDF —Å —Ä–µ–∑—é–º–µ: \").strip()\n",
    "\n",
    "    answer = system.process_query(user_text, resume_file_path=resume_path)\n",
    "    print(\"–ë–æ—Ç:\", answer)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f4582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
